{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2e125c-d6b6-4408-ad57-9037048aa45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import savgol_filter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def load_renewable_energy_data(file_path):\n",
    "    # Load and explore IoT sensor data from wind turbine SCADA system\n",
    "   \n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Dataset loaded successfully: {df.shape}\")\n",
    "        print(\" \\n IoT Sensor Data Overview \")\n",
    "        print(f\"Total Features: {df.shape[1]}\")\n",
    "        print(f\"Total Records: {df.shape[0]}\")\n",
    "        print(f\"Data Types:\\n{df.dtypes.value_counts()}\")\n",
    "        print(f\"Sample Data:\\n{df.head()}\")\n",
    "        print(f\"\\nMissing Values:\\n{df.isnull().sum()}\")\n",
    "        df = df.interpolate().fillna(method='ffill').fillna(method='bfill')\n",
    "        print(f\"\\nMissing Values after imputation: {df.isnull().sum().sum()}\")\n",
    "        required_columns = ['power_output', 'temperature', 'voltage', 'current', \n",
    "                            'vibration_x', 'vibration_y', 'rotational_speed', 'failure']\n",
    "        missing_cols = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            print(f\"Warning: Missing expected columns: {missing_cols}\")\n",
    "        print(f\"\\nClass Distribution:\\n{df['failure'].value_counts(normalize=True)}\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Dataset file not found. Please provide a valid CSV file path.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def preprocess_iot_sensor_data(df):\n",
    "    print(\"\\n= Data Preprocessing =\")\n",
    "    feature_columns = [col for col in df.columns if col not in ['failure', 'timestamp']]\n",
    "    X = df[feature_columns].copy()\n",
    "    y = df['failure'].copy()\n",
    "    for col in X.columns:\n",
    "        X[f'{col}_lag1'] = X[col].shift(1)\n",
    "        X[f'{col}_rolling_mean'] = X[col].rolling(window=5).mean()\n",
    "    X = X.fillna(X.mean())\n",
    "    print(f\"Features selected: {len(X.columns)}\")\n",
    "    print(f\"Target distribution: Normal={sum(y==0)}, Failure={sum(y==1)}\")\n",
    "    print(\"\\n1. Applying Savitzky-Golay filter\")\n",
    "    X_filtered = X.copy()\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype in ['float64', 'int64']:\n",
    "            try:\n",
    "                X_filtered[col] = savgol_filter(X[col], window_length=21, polyorder=3)\n",
    "            except:\n",
    "                print(f\"Warning: Could not apply Savitzky-Golay to {col}. Using original values.\")\n",
    "    print(\"2- Z-score outlier capping\")\n",
    "    z_scores = np.abs((X_filtered - X_filtered.mean()) / X_filtered.std())\n",
    "    outlier_mask = (z_scores > 3).any(axis=1)\n",
    "    print(f\"Outliers detected: {sum(outlier_mask)} ({sum(outlier_mask)/len(df):.2%})\")\n",
    "    for col in X_filtered.columns:\n",
    "        outlier_col_mask = z_scores[col] > 3\n",
    "        X_filtered.loc[outlier_col_mask, col] = X_filtered[col].clip(\n",
    "            lower=X_filtered[col].mean() - 3*X_filtered[col].std(),\n",
    "            upper=X_filtered[col].mean() + 3*X_filtered[col].std())\n",
    "    print(\"3. Applying StandardScaler normalization\")\n",
    "    scaler = StandardScaler()\n",
    "    X_normalized = pd.DataFrame(\n",
    "        scaler.fit_transform(X_filtered),\n",
    "        columns=X_filtered.columns,\n",
    "        index=X_filtered.index\n",
    "    )\n",
    "    print(f\"Normalized data stats: Mean={X_normalized.mean().mean():.3f}, Std={X_normalized.std().mean():.3f}\")\n",
    "    return X_normalized, y, scaler\n",
    "\n",
    "def create_time_series_features(X, y, sequence_length=5):\n",
    "    \n",
    "    # Create time series sequences for LSTM model\n",
    "    \n",
    "    print(f\"\\n=== Creating Time Series Features (sequence_length={sequence_length}) ===\")\n",
    "    if len(X) < sequence_length:\n",
    "        print(f\"Error: Dataset too small for sequence_length={sequence_length}.\")\n",
    "        return None, None, sequence_length\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(sequence_length, len(X)):\n",
    "        sequences.append(X.iloc[i-sequence_length:i].values)\n",
    "        targets.append(y.iloc[i])\n",
    "    X_sequences = np.array(sequences)\n",
    "    y_sequences = np.array(targets)\n",
    "    print(f\"Time series shape: {X_sequences.shape}\")\n",
    "    print(f\"Target shape: {y_sequences.shape}\")\n",
    "    return X_sequences, y_sequences, sequence_length\n",
    "\n",
    "def apply_smote_split(X_processed, y, X_lstm, y_lstm, sequence_length):\n",
    "    \n",
    "    # Apply SMOTE and perform temporal train-test split\n",
    "    print(\"\\nPerforming Temporal Train-Test Split\")\n",
    "    train_size = int(0.8 * len(X_processed))\n",
    "    X_train, X_test = X_processed[:train_size], X_processed[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "    X_lstm_train, X_lstm_test = X_lstm[:train_size-sequence_length], X_lstm[train_size-sequence_length:]\n",
    "    y_lstm_train, y_lstm_test = y_lstm[:train_size-sequence_length], y_lstm[train_size-sequence_length:]\n",
    "    print(\"\\nApplying SMOTE for Class Imbalance\")\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "    X_lstm_train_reshaped, y_lstm_train = smote.fit_resample(\n",
    "        X_lstm_train.reshape(X_lstm_train.shape[0], -1), y_lstm_train)\n",
    "    X_lstm_train = X_lstm_train_reshaped.reshape(-1, X_lstm.shape[1], X_lstm.shape[2])\n",
    "    print(f\"Training set: {X_train.shape}, LSTM training: {X_lstm_train.shape}\")\n",
    "    print(f\"Test set: {X_test.shape}, LSTM test: {X_lstm_test.shape}\")\n",
    "    print(f\"Post-SMOTE class distribution: Normal={sum(y_train==0)}, Failure={sum(y_train==1)}\")\n",
    "    return X_train, X_test, y_train, y_test, X_lstm_train, X_lstm_test, y_lstm_train, y_lstm_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
