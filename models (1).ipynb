{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1aefc1f-5118-4429-90c9-64c388a37ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "def build_lstm_model(input_shape):\n",
    "    \"\"\"\n",
    "    Build LSTM model with regularization\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        LSTM(200, use_bias=True, return_sequences=True, input_shape=input_shape, kernel_regularizer=l2(0.01)),\n",
    "        Dropout(0.3),\n",
    "        LSTM(250, use_bias=True, return_sequences=True, kernel_regularizer=l2(0.01)),\n",
    "        Dropout(0.4),\n",
    "        LSTM(150, use_bias=True, return_sequences=True, kernel_regularizer=l2(0.01)),\n",
    "        Dropout(0.3),\n",
    "        LSTM(100, return_sequences=False, kernel_regularizer=l2(0.01)),\n",
    "        Dropout(0.3),\n",
    "        Dense(50, kernel_regularizer=l2(0.01)),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0005),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def train_lstm_model(X_lstm_train, y_lstm_train):\n",
    "    \"\"\"\n",
    "    Train LSTM model\n",
    "    \"\"\"\n",
    "    print(\"\\n= LSTM Model Training =\")\n",
    "    lstm_model = build_lstm_model((X_lstm_train.shape[1], X_lstm_train.shape[2]))\n",
    "    print(f\"LSTM Architecture:\")\n",
    "    lstm_model.summary()\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6)\n",
    "    history = lstm_model.fit(\n",
    "        X_lstm_train, y_lstm_train,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping, lr_scheduler],\n",
    "        verbose=1\n",
    "    )\n",
    "    return lstm_model, history\n",
    "\n",
    "def train_random_forest(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Train Random Forest model with GridSearchCV\n",
    "    \"\"\"\n",
    "    print(\"\\n= Random Forest Model Training =\")\n",
    "    param_grid_rf = {\n",
    "        'n_estimators': [50, 100, 200, 300],\n",
    "        'max_depth': [5, 10, 20, 50],\n",
    "        'min_samples_split': [2, 5, 10, 20]\n",
    "    }\n",
    "    grid_search_rf = GridSearchCV(\n",
    "        RandomForestClassifier(n_estimators=50, random_state=42, class_weight='balanced'),\n",
    "        param_grid_rf, cv=5, scoring='f1', n_jobs=-1\n",
    "    )\n",
    "    grid_search_rf.fit(X_train, y_train)\n",
    "    rf_model = grid_search_rf.best_estimator_\n",
    "    print(f\"Best RF parameters: {grid_search_rf.best_params_}\")\n",
    "    rf_scores = cross_val_score(rf_model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    print(f\"Random Forest CV Accuracy: {rf_scores.mean():.3f} ± {rf_scores.std():.3f}\")\n",
    "    return rf_model\n",
    "\n",
    "def train_lightgbm(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Train LightGBM model with GridSearchCV\n",
    "    \"\"\"\n",
    "    print(\"\\n= LightGBM Model Training =\")\n",
    "    lgb_model = lgb.LGBMClassifier(random_state=42, class_weight='balanced')\n",
    "    param_grid_lgb = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [5, 10, 20],\n",
    "        'learning_rate': [0.01, 0.1, 0.2]\n",
    "    }\n",
    "    grid_search_lgb = GridSearchCV(lgb_model, param_grid_lgb, cv=5, scoring='f1', n_jobs=-1)\n",
    "    grid_search_lgb.fit(X_train, y_train)\n",
    "    lgb_model = grid_search_lgb.best_estimator_\n",
    "    print(f\"Best LightGBM parameters: {grid_search_lgb.best_params_}\")\n",
    "    lgb_scores = cross_val_score(lgb_model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    print(f\"LightGBM CV Accuracy: {lgb_scores.mean():.3f} ± {lgb_scores.std():.3f}\")\n",
    "    return lgb_model\n",
    "\n",
    "def train_xgboost(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Train XGBoost model with GridSearchCV\n",
    "    \"\"\"\n",
    "    print(\"\\n= XGBoost Model Training =\")\n",
    "    xgb_model = xgb.XGBClassifier(random_state=42, scale_pos_weight=sum(y_train==0)/sum(y_train==1))\n",
    "    param_grid_xgb = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [5, 10, 20],\n",
    "        'learning_rate': [0.01, 0.1, 0.2]\n",
    "    }\n",
    "    grid_search_xgb = GridSearchCV(xgb_model, param_grid_xgb, cv=5, scoring='f1', n_jobs=-1)\n",
    "    grid_search_xgb.fit(X_train, y_train)\n",
    "    xgb_model = grid_search_xgb.best_estimator_\n",
    "    print(f\"Best XGBoost parameters: {grid_search_xgb.best_params_}\")\n",
    "    xgb_scores = cross_val_score(xgb_model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    print(f\"XGBoost CV Accuracy: {xgb_scores.mean():.3f} ± {xgb_scores.std():.3f}\")\n",
    "    return xgb_model\n",
    "\n",
    "def build_ann_model(input_dim):\n",
    "    \"\"\"\n",
    "    Build ANN model\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_dim=input_dim, use_bias=True),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu', use_bias=True),\n",
    "        Dropout(0.3),\n",
    "        Dense(75, activation='relu', use_bias=True),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def train_ann_model(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Train ANN model\n",
    "    \"\"\"\n",
    "    print(\"\\n= ANN Model Training =\")\n",
    "    ann_model = build_ann_model(X_train.shape[1])\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6)\n",
    "    ann_history = ann_model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=50,\n",
    "        batch_size=64,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping, lr_scheduler],\n",
    "        verbose=1\n",
    "    )\n",
    "    return ann_model, ann_history\n",
    "\n",
    "def train_catboost(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Train CatBoost model with GridSearchCV\n",
    "    \"\"\"\n",
    "    print(\"\\n= CatBoost Model Training =\")\n",
    "    cat_model = CatBoostClassifier(random_state=42, verbose=0)\n",
    "    param_grid_cat = {\n",
    "        'iterations': [100, 200],\n",
    "        'depth': [4, 6, 8],\n",
    "        'learning_rate': [0.01, 0.1, 0.2]\n",
    "    }\n",
    "    grid_search_cat = GridSearchCV(cat_model, param_grid_cat, cv=5, scoring='f1', n_jobs=-1)\n",
    "    grid_search_cat.fit(X_train, y_train)\n",
    "    cat_model = grid_search_cat.best_estimator_\n",
    "    print(f\"Best CatBoost parameters: {grid_search_cat.best_params_}\")\n",
    "    cat_scores = cross_val_score(cat_model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    print(f\"CatBoost CV Accuracy: {cat_scores.mean():.3f} ± {cat_scores.std():.3f}\")\n",
    "    return cat_model\n",
    "\n",
    "def train_extra_trees(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Train Extra Trees model with GridSearchCV\n",
    "    \"\"\"\n",
    "    print(\"\\n= Extra Trees Model Training =\")\n",
    "    et_model = ExtraTreesClassifier(random_state=42, class_weight='balanced')\n",
    "    param_grid_et = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [5, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "    grid_search_et = GridSearchCV(et_model, param_grid_et, cv=5, scoring='f1', n_jobs=-1)\n",
    "    grid_search_et.fit(X_train, y_train)\n",
    "    et_model = grid_search_et.best_estimator_\n",
    "    print(f\"Best Extra Trees parameters: {grid_search_et.best_params_}\")\n",
    "    et_scores = cross_val_score(et_model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    print(f\"Extra Trees CV Accuracy: {et_scores.mean():.3f} ± {et_scores.std():.3f}\")\n",
    "    return et_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
