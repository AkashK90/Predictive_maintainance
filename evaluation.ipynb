{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb38f151-8a4b-4f1e-beb2-4a3a003e4c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(lstm_model, rf_model, lgb_model, xgb_model, ann_model, cat_model, et_model, X_test, y_test, X_lstm_test, y_lstm_test):\n",
    "    \"\"\"\n",
    "    Evaluate all models including LSTM, Random Forest, LightGBM, XGBoost, ANN, CatBoost, and Extra Trees\n",
    "    \"\"\"\n",
    "    print(\"\\n= Model Performance Evaluation =\")\n",
    "    results = {}\n",
    "    lstm_pred_prob = lstm_model.predict(X_lstm_test)\n",
    "    lstm_pred = (lstm_pred_prob > 0.5).astype(int).flatten()\n",
    "    lstm_accuracy = accuracy_score(y_lstm_test, lstm_pred)\n",
    "    rf_pred = rf_model.predict(X_test)\n",
    "    rf_pred_prob = rf_model.predict_proba(X_test)[:, 1]\n",
    "    rf_accuracy = accuracy_score(y_test, rf_pred)\n",
    "    lgb_pred = lgb_model.predict(X_test)\n",
    "    lgb_pred_prob = lgb_model.predict_proba(X_test)[:, 1]\n",
    "    lgb_accuracy = accuracy_score(y_test, lgb_pred)\n",
    "    xgb_pred = xgb_model.predict(X_test)\n",
    "    xgb_pred_prob = xgb_model.predict_proba(X_test)[:, 1]\n",
    "    xgb_accuracy = accuracy_score(y_test, xgb_pred)\n",
    "    ann_pred_prob = ann_model.predict(X_test)\n",
    "    ann_pred = (ann_pred_prob > 0.5).astype(int).flatten()\n",
    "    ann_accuracy = accuracy_score(y_test, ann_pred)\n",
    "    cat_pred = cat_model.predict(X_test)\n",
    "    cat_pred_prob = cat_model.predict_proba(X_test)[:, 1]\n",
    "    cat_accuracy = accuracy_score(y_test, cat_pred)\n",
    "    et_pred = et_model.predict(X_test)\n",
    "    et_pred_prob = et_model.predict_proba(X_test)[:, 1]\n",
    "    et_accuracy = accuracy_score(y_test, et_pred)\n",
    "    results = {\n",
    "        'LSTM': {\n",
    "            'accuracy': lstm_accuracy,\n",
    "            'predictions': lstm_pred,\n",
    "            'probabilities': lstm_pred_prob.flatten(),\n",
    "            'y_true': y_lstm_test\n",
    "        },\n",
    "        'Random Forest': {\n",
    "            'accuracy': rf_accuracy,\n",
    "            'predictions': rf_pred,\n",
    "            'probabilities': rf_pred_prob,\n",
    "            'y_true': y_test\n",
    "        },\n",
    "        'LightGBM': {\n",
    "            'accuracy': lgb_accuracy,\n",
    "            'predictions': lgb_pred,\n",
    "            'probabilities': lgb_pred_prob,\n",
    "            'y_true': y_test\n",
    "        },\n",
    "        'XGBoost': {\n",
    "            'accuracy': xgb_accuracy,\n",
    "            'predictions': xgb_pred,\n",
    "            'probabilities': xgb_pred_prob,\n",
    "            'y_true': y_test\n",
    "        },\n",
    "        'ANN': {\n",
    "            'accuracy': ann_accuracy,\n",
    "            'predictions': ann_pred,\n",
    "            'probabilities': ann_pred_prob.flatten(),\n",
    "            'y_true': y_test\n",
    "        },\n",
    "        'CatBoost': {\n",
    "            'accuracy': cat_accuracy,\n",
    "            'predictions': cat_pred,\n",
    "            'probabilities': cat_pred_prob,\n",
    "            'y_true': y_test\n",
    "        },\n",
    "        'Extra Trees': {\n",
    "            'accuracy': et_accuracy,\n",
    "            'predictions': et_pred,\n",
    "            'probabilities': et_pred_prob,\n",
    "            'y_true': y_test\n",
    "        }\n",
    "    }\n",
    "    print(\"\\n= Comparative Analysis =\")\n",
    "    print(f\"{'Model':<15} {'Accuracy (%)':12} {'Paper Target':<12}\")\n",
    "    print(\"-\" * 40)\n",
    "    for model, result in results.items():\n",
    "        target = '92%' if model == 'LSTM' else '85%' if model == 'Random Forest' else 'N/A'\n",
    "        print(f\"{model:<15} {result['accuracy']*100:<12.1f} {target:<12}\")\n",
    "    return results\n",
    "    \n",
    "def detailed_performance_analysis(results, history):\n",
    "    \"\"\"\n",
    "    Generate detailed performance metrics and visualizations\n",
    "    \"\"\"\n",
    "    print(\"\\n= Detailed Performance Analysis =\")\n",
    "    \n",
    "    for model_name, result in results.items():\n",
    "        print(f\"\\n{model_name} Classification Report:\")\n",
    "        print(classification_report(result['y_true'], result['predictions']))\n",
    "        cm = confusion_matrix(result['y_true'], result['predictions'])\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.heatmap(cm, annot=True, fmt='d')\n",
    "        plt.title(f'{model_name} Confusion Matrix')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.show()\n",
    "        try:\n",
    "            auc_score = roc_auc_score(result['y_true'], result['probabilities'])\n",
    "            print(f\"{model_name} ROC AUC Score: {auc_score:.3f}\")\n",
    "        except:\n",
    "            print(f\"{model_name} ROC AUC Score: Not available\")\n",
    "    models = list(results.keys())\n",
    "    accuracies = [results[model]['accuracy'] * 100 for model in models]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(models, accuracies, color=['blue', 'green', 'orange', 'red', 'purple', 'cyan', 'magenta'])\n",
    "    plt.title('Model Accuracy Comparison')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.ylim(0, 100)\n",
    "    for i, v in enumerate(accuracies):\n",
    "        plt.text(i, v + 0.5, f'{v:.1f}%', ha='center')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    \n",
    "\n",
    "results = evaluate_models(lstm_model, rf_model, lgb_model, xgb_model, ann_model, cat_model, et_model, X_test, y_test, X_lstm_test, y_lstm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60367807-43c5-45ea-addf-1d54a3c2e95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score\n",
    "\n",
    "def evaluate_models(lstm_model, rf_model, lgb_model, xgb_model, ann_model, cat_model, et_model, X_test, y_test, X_lstm_test, y_lstm_test):\n",
    "    \"\"\"\n",
    "    Evaluate all models including LSTM, Random Forest, LightGBM, XGBoost, ANN, CatBoost, and Extra Trees\n",
    "    \"\"\"\n",
    "    print(\"\\n= Model Performance Evaluation =\")\n",
    "    results = {}\n",
    "    lstm_pred_prob = lstm_model.predict(X_lstm_test)\n",
    "    lstm_pred = (lstm_pred_prob > 0.6).astype(int).flatten()\n",
    "    lstm_accuracy = accuracy_score(y_lstm_test, lstm_pred)\n",
    "    rf_pred = rf_model.predict(X_test)\n",
    "    rf_pred_prob = rf_model.predict_proba(X_test)[:, 1]\n",
    "    rf_accuracy = accuracy_score(y_test, rf_pred)\n",
    "    lgb_pred = lgb_model.predict(X_test)\n",
    "    lgb_pred_prob = lgb_model.predict_proba(X_test)[:, 1]\n",
    "    lgb_accuracy = accuracy_score(y_test, lgb_pred)\n",
    "    xgb_pred = xgb_model.predict(X_test)\n",
    "    xgb_pred_prob = xgb_model.predict_proba(X_test)[:, 1]\n",
    "    xgb_accuracy = accuracy_score(y_test, xgb_pred)\n",
    "    ann_pred_prob = ann_model.predict(X_test)\n",
    "    ann_pred = (ann_pred_prob > 0.5).astype(int).flatten()\n",
    "    ann_accuracy = accuracy_score(y_test, ann_pred)\n",
    "    cat_pred = cat_model.predict(X_test)\n",
    "    cat_pred_prob = cat_model.predict_proba(X_test)[:, 1]\n",
    "    cat_accuracy = accuracy_score(y_test, cat_pred)\n",
    "    et_pred = et_model.predict(X_test)\n",
    "    et_pred_prob = et_model.predict_proba(X_test)[:, 1]\n",
    "    et_accuracy = accuracy_score(y_test, et_pred)\n",
    "    results = {\n",
    "        'LSTM': {\n",
    "            'accuracy': lstm_accuracy,\n",
    "            'predictions': lstm_pred,\n",
    "            'probabilities': lstm_pred_prob.flatten(),\n",
    "            'y_true': y_lstm_test\n",
    "        },\n",
    "        'Random Forest': {\n",
    "            'accuracy': rf_accuracy,\n",
    "            'predictions': rf_pred,\n",
    "            'probabilities': rf_pred_prob,\n",
    "            'y_true': y_test\n",
    "        },\n",
    "        'LightGBM': {\n",
    "            'accuracy': lgb_accuracy,\n",
    "            'predictions': lgb_pred,\n",
    "            'probabilities': lgb_pred_prob,\n",
    "            'y_true': y_test\n",
    "        },\n",
    "        'XGBoost': {\n",
    "            'accuracy': xgb_accuracy,\n",
    "            'predictions': xgb_pred,\n",
    "            'probabilities': xgb_pred_prob,\n",
    "            'y_true': y_test\n",
    "        },\n",
    "        'ANN': {\n",
    "            'accuracy': ann_accuracy,\n",
    "            'predictions': ann_pred,\n",
    "            'probabilities': ann_pred_prob.flatten(),\n",
    "            'y_true': y_test\n",
    "        },\n",
    "        'CatBoost': {\n",
    "            'accuracy': cat_accuracy,\n",
    "            'predictions': cat_pred,\n",
    "            'probabilities': cat_pred_prob,\n",
    "            'y_true': y_test\n",
    "        },\n",
    "        'Extra Trees': {\n",
    "            'accuracy': et_accuracy,\n",
    "            'predictions': et_pred,\n",
    "            'probabilities': et_pred_prob,\n",
    "            'y_true': y_test\n",
    "        }\n",
    "    }\n",
    "    print(\"\\n= Comparative Analysis =\")\n",
    "    print(f\"{'Model':<15} {'Accuracy (%)':12} {'Paper Target':<12}\")\n",
    "    print(\"-\" * 40)\n",
    "    for model, result in results.items():\n",
    "        target = '92%' if model == 'LSTM' else '85%' if model == 'Random Forest' else 'N/A'\n",
    "        print(f\"{model:<15} {result['accuracy']*100:<12.1f} {target:<12}\")\n",
    "    return results\n",
    "\n",
    "def detailed_performance_analysis(results, history):\n",
    "    \"\"\"\n",
    "    Generate detailed performance metrics and visualizations\n",
    "    \"\"\"\n",
    "    print(\"\\n= Detailed Performance Analysis =\")\n",
    "    for model_name, result in results.items():\n",
    "        print(f\"\\n{model_name} Classification Report:\")\n",
    "        print(classification_report(result['y_true'], result['predictions']))\n",
    "        cm = confusion_matrix(result['y_true'], result['predictions'])\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.heatmap(cm, annot=True, fmt='d')\n",
    "        plt.title(f'{model_name} Confusion Matrix')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.show()\n",
    "        try:\n",
    "            auc_score = roc_auc_score(result['y_true'], result['probabilities'])\n",
    "            print(f\"{model_name} ROC AUC Score: {auc_score:.3f}\")\n",
    "        except:\n",
    "            print(f\"{model_name} ROC AUC Score: Not available\")\n",
    "    models = list(results.keys())\n",
    "    accuracies = [results[model]['accuracy'] * 100 for model in models]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(models, accuracies, color=['blue', 'green', 'orange', 'red', 'purple', 'cyan', 'magenta'])\n",
    "    plt.title('Model Accuracy Comparison')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.ylim(0, 100)\n",
    "    for i, v in enumerate(accuracies):\n",
    "        plt.text(i, v + 0.5, f'{v:.1f}%', ha='center')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('LSTM Training History')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
